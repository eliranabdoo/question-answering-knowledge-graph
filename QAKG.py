# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eO9MXaX9MDMskOCOXLQSU_w7Srrl69TH

# Sentence-BERT
"""

!pip install scipy==1.7.0
!pip install torch transformers pandas datasets matplotlib pydantic faiss-gpu

"""### Config"""

from pydantic.fields import Field
from pydantic import BaseModel
import torch

CUDA = 'cuda'
CPU = 'cpu'

class FeaturesNames(BaseModel):
    title: str = "title"
    embedding: str = "embedding"
    token_embedding: str = "token_embedding"
    content: str = "content"
    raw_content: str = "raw_content"
    token_ids: str = "input_ids"
    attention_mask: str = "attention_mask"


class Config(BaseModel):
  data_path: str = None  # set to data
  checkpoint: str = "sentence-transformers/distiluse-base-multilingual-cased-v2"
  features_names: FeaturesNames = Field(default_factory=FeaturesNames)
  device: str = CUDA if torch.cuda.is_available() else CPU
  batch_size: int = 100
  test_mode: bool = False
  seed: int = 1337
  beam_size: int = 5


config = Config()

"""### Data Loading"""

from datasets import Dataset, load_dataset
import pandas as pd
def load_data(csv_path) -> Dataset:
  df = pd.read_csv(csv_path, encoding="utf-8")
  dataset = Dataset.from_pandas(df)
  dataset = dataset.rename_column("sentence", config.features_names.raw_content)
  return dataset

"""### Data Preprocessing"""

from typing import List
import string

PUNCT_REMOVAL_DICT = {ord(c): None for c in string.punctuation if c != ','}

def preprocess_sentences(sentences: List[str]) -> List[str]:
  sentences = list(map(lambda sentence: ' '.join(sentence.split()).strip(), sentences))
  sentences = list(map(lambda sentence: sentence.translate(PUNCT_REMOVAL_DICT), sentences))
  sentences = list(set(sentences))
  return sentences

def preprocess_data(data: Dataset) -> Dataset:
  return data.map(lambda batch: {config.features_names.content: preprocess_sentences(batch[config.features_names.raw_content])},
                  batched=True, batch_size=config.batch_size, remove_columns=config.features_names.raw_content)

"""### Embeddings"""

import torch 

def batched_tokenize(tokenizer):
    def apply(batch):
        res = tokenizer(batch[config.features_names.content], truncation=True, return_tensors="pt", padding=True)
        return {k: v.detach().cpu().numpy() for k, v in res.items()}
    return apply

def batched_inference(model):
    def apply(batch):
        with torch.no_grad():
            outputs = model(torch.Tensor(batch[config.features_names.token_ids]).int().to(config.device),
                            attention_mask=torch.Tensor(batch[config.features_names.attention_mask]).to(config.device))
        res = {config.features_names.token_embedding: outputs.last_hidden_state.detach().cpu().numpy()}
        return res
    return apply

def batched_mean_pooling(batch):
    token_embedding = torch.Tensor(batch[config.features_names.token_embedding])
    attention_mask = torch.Tensor(batch[config.features_names.attention_mask])
    attention_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embedding.size()).float()
    sentence_embeddings = torch.sum(token_embedding * attention_mask_expanded, 1) / torch.clamp(attention_mask_expanded.sum(1),
                                                                                   min=1e-9)
    return {config.features_names.embedding: sentence_embeddings.numpy()}

def add_sentence_embeddings(dataset: Dataset, model, tokenizer) -> Dataset:
  dataset = dataset.map(batched_tokenize(tokenizer), batched=True, batch_size=config.batch_size)
  model.to(config.device)
  dataset = dataset.map(batched_inference(model), batched=True, batch_size=config.batch_size)
  model.to(CPU)
  dataset = dataset.map(batched_mean_pooling, batched=True, batch_size=config.batch_size)
  dataset = dataset.remove_columns([config.features_names.token_ids,
                                    config.features_names.attention_mask,
                                    config.features_names.token_embedding])
  return dataset

"""### Indexing"""

def add_faiss_index(dataset: Dataset) -> Dataset:
  dataset.add_faiss_index(column=config.features_names.embedding)
  return dataset

"""### Searching"""

import numpy as np
from scipy.special import softmax
from pydantic import validator

class QnA(BaseModel):
  question: str
  answers: List[str]
  scores: List[float]

  @validator('scores')
  def apply_softmax(cls, v):
    return softmax(v)

  def __str__(self):
    return "\n".join([self.question] + [f"{a} | {s:.2f}" for a,s in zip(self.answers, self.scores)])

def queries_to_dataset(queries: List[str], model, tokenizer) -> Dataset:
  queries_df = pd.DataFrame({config.features_names.raw_content: queries})
  queries_dataset = Dataset.from_pandas(queries_df)
  del queries_df
  queries_dataset = preprocess_data(queries_dataset)
  queries_dataset = add_sentence_embeddings(queries_dataset, model, tokenizer)
  return queries_dataset

def search(queries: List[str], tokenizer, model, indexed_dataset: Dataset, top_k: int) -> List[QnA]:
  qnas = []
  top_k = min(top_k, len(indexed_dataset))

  queries_dataset = queries_to_dataset(queries, model, tokenizer)
  for query, query_embedding in zip(queries, queries_dataset[config.features_names.embedding]):
    scores, samples = indexed_dataset.get_nearest_examples(
      config.features_names.embedding, np.asarray(query_embedding).astype(np.float32), k=top_k
    )

    answers = samples[config.features_names.content]
    logits = -1*scores

    qnas.append(QnA(question=query,
                    answers=answers,
                    scores=list(logits)))
  return qnas

"""## Get The Answers (Part I)

### Load Model
"""

from transformers import AutoTokenizer, AutoModel
model, tokenizer = AutoModel.from_pretrained(config.checkpoint), AutoTokenizer.from_pretrained(config.checkpoint)

"""### Load Data"""

from datasets import Dataset
import pandas as pd
if config.test_mode:
  data = Dataset.from_pandas(pd.DataFrame({config.features_names.raw_content: ["Hey there", "It was me who supervises", "I have nothing to do with this", "The thing that was added to the recording registry? Nathan overseered it", "bla bla supervised bla bla national recording registry bla bla"]}))
else:
  data = load_data(config.data_path)

"""### Create Embeddings Index"""

data = preprocess_data(data)
data = add_sentence_embeddings(data, model=model, tokenizer=tokenizer)
data = add_faiss_index(data)

"""### Query"""

the_query = "Who supervised the thing that was added to the national recording registry?"
print(search([the_query], tokenizer=tokenizer, model=model, indexed_dataset=data, top_k=5)[0])

"""# Graph Analytics"""

!pip install --upgrade pydantic spacy
!python -m spacy download en_core_web_sm

"""### Dependency Tree Names"""

from pydantic.fields import Field
from pydantic import BaseModel

class DepTreeComponents(BaseModel):
    root: str = "root"
    head: str = "head"
    tail: str = "tail"
    root_prep: str = "root_prep"

dep_config = DepTreeComponents()

"""### Data Model"""

from dataclasses import dataclass

@dataclass
class Triplet:
  head: str
  relation: str
  tail: str
  origin: str

"""### Parsing functions"""

from typing import Union, Dict, List

def get_full_noun_chunks(doc, partial_nouns):
  not_found = {pn:pn for pn in partial_nouns}
  found = {}
  for noun_chunk in doc.noun_chunks:
      for partial_noun in list(not_found.keys()):
        if partial_noun in noun_chunk:
          found[partial_noun] = noun_chunk
          del not_found[partial_noun]
  return {**not_found, **found}

def process_triplet_params(triplet_params, doc) -> Dict[str, str]:
  if not isinstance(triplet_params[RELATION], tuple):
    triplet_params[RELATION] = (triplet_params[RELATION],)
  triplet_params[RELATION] = ' '.join([t.lemma_ for t in triplet_params[RELATION]])
  expansion_keys = [HEAD, TAIL]
  expanded_params = get_full_noun_chunks(doc, [triplet_params[k] for k in expansion_keys])
  for k in expansion_keys:
    triplet_params[k] = expanded_params[triplet_params[k]]
    triplet_params[k] = triplet_params[k].text
  return {**triplet_params, **expanded_params}


def get_triplet_from_match(match_pattern, match_token_ids, doc, match_priorities) -> Union[None, Triplet]:
  raw_head, raw_tail, raw_action = None, None, None
  dep_id_to_token = {token_pattern[RIGHT_ID]: doc[token_id] for token_pattern, token_id in zip(match_pattern, match_token_ids)}

  triplet_params = {}
  for triplet_key, match_keys in match_priorities.items():
    triplet_value = None
    for match_key in match_keys:
      try:
        if isinstance(match_key, tuple):
          triplet_value = tuple(map(lambda k: dep_id_to_token[k], match_key))
        elif isinstance(match_key, str):
          triplet_value = dep_id_to_token[match_key]
      except KeyError:
        pass
      else:
        break
    if triplet_value is None:  # no hope for this triplet
      return None
    triplet_params[triplet_key] = triplet_value
  process_triplet_params(triplet_params, doc)
  return Triplet(origin=doc.text, **triplet_params)

  
def get_triplets_from_sentence(sent: str, matcher, model, match_priorities, match_filter=None) -> List[Triplet]:
  doc = model(sent)
  matches = matcher(doc)
  triplets = []
  if match_filter is not None:
    matches = match_filter(matches)
  for match_id, token_ids in matches:
    match_pattern = matcher.get(match_id)[1][0]
    triplet = get_triplet_from_match(match_pattern, token_ids, doc, match_priorities)
    if triplet is not None:
      triplets.append(triplet)
  return triplets

def take_first_match_filter(matches):
  if len(matches) == 0:
    return matches
  first_id = matches[0][0]
  return list(filter(lambda match: match[0] == first_id, matches))

"""### Data loading"""

import pandas as pd

def load_data(csv_path) -> List[str]:
  df = pd.read_csv(csv_path, encoding="utf-8")
  return list(df["sentence"])

"""### Graph generation"""

import networkx as nx
def generate_graph_from_triplets(triplets: List[Triplet]):
  graph = nx.DiGraph()
  graph.add_edges_from(((t.head, t.tail, {RELATION: t.relation}) for t in triplets))
  return graph

"""## Get the answers (Part II)

### Lingual Triplet Pattern
"""

# consts #
RIGHT_ID = "RIGHT_ID"
LEFT_ID = "LEFT_ID"
RIGHT_ATTRS = "RIGHT_ATTRS"
REL_OP = "REL_OP"
HEAD = "head"
TAIL = "tail"
RELATION = "relation"

ROOT =  {"POS": {"IN": ["VERB", "AUX"]}, "DEP": "ROOT"}
NOUN_POS = {"POS": {"IN":["NOUN", "PROPN"]}}

# patterns #
patterns = [[
  {
    RIGHT_ID: dep_config.root,
    RIGHT_ATTRS: ROOT
  },
  {
    LEFT_ID: dep_config.root,
    REL_OP: ">",
    RIGHT_ID: dep_config.root_prep,
    RIGHT_ATTRS: {"DEP": "prep"}
  },
  {
    LEFT_ID: dep_config.root,
    REL_OP: ";*",
    RIGHT_ID: dep_config.head,
    RIGHT_ATTRS: {"DEP": {"IN": ["nsubj", "conj", "nsubjpass"]}, **NOUN_POS}
  },
  {
    LEFT_ID: dep_config.root_prep,
    REL_OP: ">>",
    RIGHT_ID: dep_config.tail,
    RIGHT_ATTRS: {"DEP": {"IN": ["dobj", "conj", "pobj"]}, **NOUN_POS}
  }
],
[
  {
    RIGHT_ID: dep_config.root,
    RIGHT_ATTRS: ROOT
  },
  {
    LEFT_ID: dep_config.root,
    REL_OP: ";*",
    RIGHT_ID: dep_config.head,
    RIGHT_ATTRS: {"DEP": {"IN": ["nsubj", "conj"]}, **NOUN_POS}
  },
  {
    LEFT_ID: dep_config.root,
    REL_OP: ".*",
    RIGHT_ID: dep_config.tail,
    RIGHT_ATTRS: {"DEP": {"IN": ["dobj", "conj", "attr"]}, **NOUN_POS}
  }
],
[
  {
    RIGHT_ID: dep_config.root,
    RIGHT_ATTRS: ROOT
  },
  {
    LEFT_ID: dep_config.root,
    REL_OP: ";*",
    RIGHT_ID: dep_config.tail,
    RIGHT_ATTRS: {"DEP": "nsubjpass", **NOUN_POS}
  },
  {
    LEFT_ID: dep_config.root,
    REL_OP: ".*",
    RIGHT_ID: dep_config.head,
    RIGHT_ATTRS: {"DEP": "pobj", **NOUN_POS}
  }
]
]

match_priorities = {
   HEAD: [dep_config.head],
   TAIL: [dep_config.tail],
   RELATION: [(dep_config.root, dep_config.root_prep), dep_config.root]
}

"""### Load model"""

import spacy
model = spacy.load("en_core_web_sm")

"""### Load sentences"""

raw_sentences = load_data(config.data_path)
sentences = preprocess_sentences(raw_sentences)

"""### Create the matcher"""

from spacy.matcher import DependencyMatcher
matcher = DependencyMatcher(model.vocab)
for i, p in enumerate(patterns):
  matcher.add(i, [p])

"""### Generate the triplets"""

from tqdm import tqdm
from multiprocessing import Pool
import multiprocessing

with Pool(multiprocessing.cpu_count()) as p:
  triplets_per_sent = p.starmap(get_triplets_from_sentence, tqdm(((sent, matcher, model, match_priorities,take_first_match_filter) for sent in sentences), total=len(sentences)))
all_triplets = [t for sent_triplets in triplets_per_sent for t in sent_triplets]

"""### Generate the graph"""

graph = generate_graph_from_triplets(all_triplets)

"""### Draw"""

from networkx import ego_graph
from matplotlib import pyplot as plt
plt.figure(0, figsize=(10,10))

root = [n for n in graph.nodes if "national recording registry" in n][0]
subgraph = ego_graph(graph.reverse(), root, radius=2)

pos = nx.spring_layout(subgraph, seed=config.seed)
fig = nx.draw(subgraph, pos=pos ,node_color="b", node_size=50, with_labels=True)

options = {"node_size": 300, "node_color": "r"}
nx.draw_networkx_nodes(subgraph, pos, nodelist=[root], **options)

edge_labels = nx.get_edge_attributes(subgraph,'relation')
_ = nx.draw_networkx_edge_labels(subgraph, pos=pos, edge_labels=edge_labels)

"""# Representation Learning"""

!pip install pykeen

"""### Load triplets to pykeen dataset"""

from pykeen.pipeline import pipeline
from pykeen.triples import TriplesFactory

triplets_array = pd.DataFrame.from_records([vars(triplet) for triplet in all_triplets]).values
training = TriplesFactory.from_labeled_triples(triplets_array, create_inverse_triples=False)

"""### Train model"""

result = pipeline(
    training=training,
    testing=training,
    model='TransE',
    epochs=100,
    random_seed=config.seed
)
model = result.model

"""### Assisted Beam Search"""

import heapq
from collections import namedtuple
from pykeen.models import predict

Beam = namedtuple("Beam", ["score", "elements"])

def assisted_beam_search(score_generator, initial_inputs,
                         next_input_generator, beam_size):
  beams = [Beam(score=0.0, elements=initial_inputs) for _ in range(beam_size)]
  for next_input in next_input_generator:
    new_beams = []
    for beam in beams:
      assisted_beam_elements = beam.elements + next_input
      beam_extentions = score_generator(assisted_beam_elements, beam_size)
      candidates = (Beam(elements=assisted_beam_elements + extention.elements,
                         score=beam.score + extention.score)
       for extention in beam_extentions)
      for candidate in candidates:
        if candidate not in new_beams:
          if len(new_beams) == beam_size:
            heapq.heapreplace(new_beams, candidate)
          else:
            heapq.heappush(new_beams,candidate)     
    beams = new_beams
  return sorted(beams, reverse=True)

def get_top_k_sorted_prediction(prediction_df: pd.DataFrame, k, label_col, score_col):
  return [Beam(score=getattr(t,score_col), elements=(getattr(t,label_col),))
   for t in prediction_df.nlargest(k, columns=score_col)[[label_col, score_col]].itertuples()]

def get_heads_score_generator(pykeen_model, triples_factory):
  def score_generator(elements, top_k):
    tail, relation = elements[-2], elements[-1]
    preds_df = predict.get_head_prediction_df(
    pykeen_model, relation , tail, triples_factory=triples_factory)
    preds_df = preds_df[preds_df["head_label"] != tail]
    top_k_preds = get_top_k_sorted_prediction(preds_df, k=top_k, label_col="head_label", score_col="score")
    return top_k_preds
  return score_generator

"""## Get The Answers (Part III)"""

assisted_beam_search(get_heads_score_generator(model, triples_factory=training),
            ("the united states national recording registry",),
            (("add to", ), ("supervise", )),
            beam_size=config.beam_size)
